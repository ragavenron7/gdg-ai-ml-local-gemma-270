{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to KaggleHub\n",
    "The code below imports the `kagglehub` library and authenticates your session so you can access datasets, models, and other KaggleHub resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and GPU Configuration \n",
    "\n",
    "This cell prepares the runtime for fine-tuning.\n",
    "We set the Keras backend to TensorFlow, specify which GPUs should be visible, enable dynamic GPU memory growth to avoid full memory reservation, and suppress unnecessary TensorFlow logs.\n",
    "\n",
    "After applying these settings, we import TensorFlow, check how many GPUs are available, and enable memory-growth on each one. This ensures stable GPU usage when loading and training the Gemma 3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:04.81314Z",
     "iopub.status.busy": "2025-12-07T11:48:04.812914Z",
     "iopub.status.idle": "2025-12-07T11:48:27.842182Z",
     "shell.execute_reply": "2025-12-07T11:48:27.841484Z",
     "shell.execute_reply.started": "2025-12-07T11:48:04.813116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Import TensorFlow FIRST to lock in GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Initial GPU check: {len(gpus)} GPUs\")\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"✓ GPUs configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrade KerasNLP to the Latest Version\n",
    "\n",
    "This cell installs the latest version of KerasNLP, which includes full support for the Gemma 3 model family.\n",
    "We run a simple pip upgrade command, and then print a confirmation.\n",
    "Do not restart the runtime after installing, because TensorFlow and the GPU setup from the previous cell would reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:27.844005Z",
     "iopub.status.busy": "2025-12-07T11:48:27.843544Z",
     "iopub.status.idle": "2025-12-07T11:48:35.01731Z",
     "shell.execute_reply": "2025-12-07T11:48:35.016567Z",
     "shell.execute_reply.started": "2025-12-07T11:48:27.843986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Upgrade to latest KerasNLP for Gemma3 support\n",
    "!pip install -q --upgrade keras-nlp\n",
    "!\n",
    "\n",
    "print(\"✓ KerasNLP upgraded to latest - continue to next cell (do NOT restart)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Installation and Environment\n",
    "\n",
    "This cell performs several checks before we start fine-tuning:\n",
    "\n",
    "\n",
    "1. Imports required libraries: keras, keras_nlp, TopKSampler, time, csv, and logging.\n",
    "2. Suppresses verbose logs from sentencepiece.\n",
    "3. Prints the current versions of Keras and KerasNLP.\n",
    "4. Re-checks that GPUs are still available.\n",
    "5. Verifies that the Gemma3CausalLM model is present in KerasNLP.\n",
    "\n",
    "\n",
    "This ensures the environment is correctly set up and ready for model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:35.018398Z",
     "iopub.status.busy": "2025-12-07T11:48:35.018187Z",
     "iopub.status.idle": "2025-12-07T11:48:36.913853Z",
     "shell.execute_reply": "2025-12-07T11:48:36.91318Z",
     "shell.execute_reply.started": "2025-12-07T11:48:35.018377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.samplers import TopKSampler\n",
    "from time import time\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "# Suppress messages\n",
    "logging.getLogger(\"sentencepiece\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "\n",
    "# Re-verify GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Num GPUs: {len(gpus)}\")\n",
    "\n",
    "if gpus:\n",
    "    print(\"✓✓✓ GPU STILL DETECTED! ✓✓✓\")\n",
    "else:\n",
    "    print(\"⚠️ GPU lost\")\n",
    "    \n",
    "# Check Gemma3\n",
    "if hasattr(keras_nlp.models, 'Gemma3CausalLM'):\n",
    "    print(\"✓ Gemma3CausalLM available!\")\n",
    "else:\n",
    "    print(\"✗ Gemma3CausalLM NOT available\")\n",
    "    print(f\"Available: {[x for x in dir(keras_nlp.models) if 'Gemma' in x]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Gemma3 270M Model\n",
    "\n",
    "This cell loads the Gemma3 270M causal language model using KerasNLP’s from_preset method.\n",
    "We use the Kaggle-hosted preset to get the pre-trained weights and configuration.\n",
    "Once loaded, the model is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.model_download(\"keras/gemma3/keras/gemma3_270m\")\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:36.914952Z",
     "iopub.status.busy": "2025-12-07T11:48:36.914693Z",
     "iopub.status.idle": "2025-12-07T11:48:48.784169Z",
     "shell.execute_reply": "2025-12-07T11:48:48.783459Z",
     "shell.execute_reply.started": "2025-12-07T11:48:36.914919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# We load the model gemma_3_270M using keras_nlp.\n",
    "print(\"Loading Gemma3 270M model...\")\n",
    "gemma_lm = keras_nlp.models.Gemma3CausalLM.from_preset(path)\n",
    "print(\"✓ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Dataset\n",
    "\n",
    "This cell reads a CSV file containing medical question-answer pairs and converts it into a format suitable for fine-tuning.\n",
    "\n",
    "The CSV has two columns: question and answer.\n",
    "\n",
    "Each row is transformed into a dictionary with keys prompts (from question) and responses (from answer).\n",
    "\n",
    "All examples are collected in a list called data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:48.817436Z",
     "iopub.status.busy": "2025-12-07T11:48:48.817137Z",
     "iopub.status.idle": "2025-12-07T11:48:49.23186Z",
     "shell.execute_reply": "2025-12-07T11:48:49.23123Z",
     "shell.execute_reply.started": "2025-12-07T11:48:48.817419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"gpreda/medquad\")\n",
    "print(\"Dataset downloaded to:\", path)\n",
    "\n",
    "csv_path = f\"{path}/medquad.csv\"      \n",
    "\n",
    "data = []\n",
    "\n",
    "# The CSV file contains two columns 'question' and 'answer'\n",
    "with open(csv_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # we replace with 'prompts' and 'responses'\n",
    "        data.append({\"prompts\": row['question'], 'responses': row['answer']})"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5314260,
     "sourceId": 8831896,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 279036,
     "modelInstanceId": 410137,
     "sourceId": 521980,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
