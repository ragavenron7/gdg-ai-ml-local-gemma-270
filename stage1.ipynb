{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and GPU Configuration \n",
    "\n",
    "This cell prepares the runtime for fine-tuning.\n",
    "We set the Keras backend to TensorFlow, specify which GPUs should be visible, enable dynamic GPU memory growth to avoid full memory reservation, and suppress unnecessary TensorFlow logs.\n",
    "\n",
    "After applying these settings, we import TensorFlow, check how many GPUs are available, and enable memory-growth on each one. This ensures stable GPU usage when loading and training the Gemma 3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:04.81314Z",
     "iopub.status.busy": "2025-12-07T11:48:04.812914Z",
     "iopub.status.idle": "2025-12-07T11:48:27.842182Z",
     "shell.execute_reply": "2025-12-07T11:48:27.841484Z",
     "shell.execute_reply.started": "2025-12-07T11:48:04.813116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Import TensorFlow FIRST to lock in GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Initial GPU check: {len(gpus)} GPUs\")\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"✓ GPUs configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrade KerasNLP to the Latest Version\n",
    "\n",
    "This cell installs the latest version of KerasNLP, which includes full support for the Gemma 3 model family.\n",
    "We run a simple pip upgrade command, and then print a confirmation.\n",
    "Do not restart the runtime after installing, because TensorFlow and the GPU setup from the previous cell would reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:27.844005Z",
     "iopub.status.busy": "2025-12-07T11:48:27.843544Z",
     "iopub.status.idle": "2025-12-07T11:48:35.01731Z",
     "shell.execute_reply": "2025-12-07T11:48:35.016567Z",
     "shell.execute_reply.started": "2025-12-07T11:48:27.843986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Upgrade to latest KerasNLP for Gemma3 support\n",
    "!pip install -q --upgrade keras-nlp\n",
    "!\n",
    "\n",
    "print(\"✓ KerasNLP upgraded to latest - continue to next cell (do NOT restart)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Installation and Environment\n",
    "\n",
    "This cell performs several checks before we start fine-tuning:\n",
    "\n",
    "\n",
    "1. Imports required libraries: keras, keras_nlp, TopKSampler, time, csv, and logging.\n",
    "2. Suppresses verbose logs from sentencepiece.\n",
    "3. Prints the current versions of Keras and KerasNLP.\n",
    "4. Re-checks that GPUs are still available.\n",
    "5. Verifies that the Gemma3CausalLM model is present in KerasNLP.\n",
    "\n",
    "\n",
    "This ensures the environment is correctly set up and ready for model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:35.018398Z",
     "iopub.status.busy": "2025-12-07T11:48:35.018187Z",
     "iopub.status.idle": "2025-12-07T11:48:36.913853Z",
     "shell.execute_reply": "2025-12-07T11:48:36.91318Z",
     "shell.execute_reply.started": "2025-12-07T11:48:35.018377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.samplers import TopKSampler\n",
    "from time import time\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "# Suppress messages\n",
    "logging.getLogger(\"sentencepiece\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "\n",
    "# Re-verify GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Num GPUs: {len(gpus)}\")\n",
    "\n",
    "if gpus:\n",
    "    print(\"✓✓✓ GPU STILL DETECTED! ✓✓✓\")\n",
    "else:\n",
    "    print(\"⚠️ GPU lost\")\n",
    "    \n",
    "# Check Gemma3\n",
    "if hasattr(keras_nlp.models, 'Gemma3CausalLM'):\n",
    "    print(\"✓ Gemma3CausalLM available!\")\n",
    "else:\n",
    "    print(\"✗ Gemma3CausalLM NOT available\")\n",
    "    print(f\"Available: {[x for x in dir(keras_nlp.models) if 'Gemma' in x]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Gemma3 270M Model\n",
    "\n",
    "This cell loads the Gemma3 270M causal language model using KerasNLP’s from_preset method.\n",
    "We use the Kaggle-hosted preset to get the pre-trained weights and configuration.\n",
    "Once loaded, the model is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.model_download(\"keras/gemma3/keras/gemma3_270m\")\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:36.914952Z",
     "iopub.status.busy": "2025-12-07T11:48:36.914693Z",
     "iopub.status.idle": "2025-12-07T11:48:48.784169Z",
     "shell.execute_reply": "2025-12-07T11:48:48.783459Z",
     "shell.execute_reply.started": "2025-12-07T11:48:36.914919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# We load the model gemma_3_270M using keras_nlp.\n",
    "print(\"Loading Gemma3 270M model...\")\n",
    "gemma_lm = keras_nlp.models.Gemma3CausalLM.from_preset(path)\n",
    "print(\"✓ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Model Architecture\n",
    "\n",
    "This cell displays a summary of the Gemma3 270M model, including:\n",
    "\n",
    "Layer types\n",
    "\n",
    "Output shapes\n",
    "\n",
    "Number of parameters\n",
    "\n",
    "It helps us understand the model structure and verify that it loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:48.785176Z",
     "iopub.status.busy": "2025-12-07T11:48:48.784923Z",
     "iopub.status.idle": "2025-12-07T11:48:48.809103Z",
     "shell.execute_reply": "2025-12-07T11:48:48.808355Z",
     "shell.execute_reply.started": "2025-12-07T11:48:48.785157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Configuration\n",
    "\n",
    "This cell sets up a simple configuration class CFG that contains key hyperparameters for fine-tuning:\n",
    "\n",
    "\n",
    "* max_length: Maximum sequence length for input text.\n",
    "* data_size: Number of training examples to use.\n",
    "* lora_rank: Rank for LoRA (Low-Rank Adaptation) fine-tuning.\n",
    "* epochs: Number of training epochs.\n",
    "* batch_size: Number of samples per training batch.\n",
    "\n",
    "\n",
    "An instance cfg is created so these parameters can be easily accessed throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:48.810658Z",
     "iopub.status.busy": "2025-12-07T11:48:48.80978Z",
     "iopub.status.idle": "2025-12-07T11:48:48.815091Z",
     "shell.execute_reply": "2025-12-07T11:48:48.81459Z",
     "shell.execute_reply.started": "2025-12-07T11:48:48.81063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    " \n",
    "    max_length = 128\n",
    "    data_size = 2560\n",
    "    lora_rank = 16\n",
    "    epochs = 40\n",
    "    batch_size = 2\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Dataset\n",
    "\n",
    "This cell reads a CSV file containing medical question-answer pairs and converts it into a format suitable for fine-tuning.\n",
    "\n",
    "The CSV has two columns: question and answer.\n",
    "\n",
    "Each row is transformed into a dictionary with keys prompts (from question) and responses (from answer).\n",
    "\n",
    "All examples are collected in a list called data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:48.817436Z",
     "iopub.status.busy": "2025-12-07T11:48:48.817137Z",
     "iopub.status.idle": "2025-12-07T11:48:49.23186Z",
     "shell.execute_reply": "2025-12-07T11:48:49.23123Z",
     "shell.execute_reply.started": "2025-12-07T11:48:48.817419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"gpreda/medquad\")\n",
    "print(\"Dataset downloaded to:\", path)\n",
    "\n",
    "csv_path = f\"{path}/medquad.csv\"      \n",
    "\n",
    "data = []\n",
    "\n",
    "# The CSV file contains two columns 'question' and 'answer'\n",
    "with open(csv_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # we replace with 'prompts' and 'responses'\n",
    "        data.append({\"prompts\": row['question'], 'responses': row['answer']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.232838Z",
     "iopub.status.busy": "2025-12-07T11:48:49.232537Z",
     "iopub.status.idle": "2025-12-07T11:48:49.237315Z",
     "shell.execute_reply": "2025-12-07T11:48:49.236561Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.232812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Data size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit Dataset Size\n",
    "\n",
    "This cell trims the dataset to the first cfg.data_size examples.\n",
    "This allows faster training and easier experimentation while still using a representative subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.23899Z",
     "iopub.status.busy": "2025-12-07T11:48:49.238754Z",
     "iopub.status.idle": "2025-12-07T11:48:49.257987Z",
     "shell.execute_reply": "2025-12-07T11:48:49.257456Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.238973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = data[:cfg.data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.258958Z",
     "iopub.status.busy": "2025-12-07T11:48:49.258775Z",
     "iopub.status.idle": "2025-12-07T11:48:49.274461Z",
     "shell.execute_reply": "2025-12-07T11:48:49.273762Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.258944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Data size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data to TensorFlow Dataset\n",
    "\n",
    "This cell converts the Python list data into a TensorFlow tf.data.Dataset, which is optimized for training.\n",
    "\n",
    "We use a generator to yield each dictionary from data.\n",
    "\n",
    "output_signature specifies the expected shape and type for each field: both prompts and responses are strings.\n",
    "\n",
    "This allows TensorFlow to efficiently batch, shuffle, and prefetch the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.275569Z",
     "iopub.status.busy": "2025-12-07T11:48:49.275259Z",
     "iopub.status.idle": "2025-12-07T11:48:49.325191Z",
     "shell.execute_reply": "2025-12-07T11:48:49.324622Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.275551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: (item for item in data),\n",
    "    output_signature={\n",
    "        \"prompts\": tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "        \"responses\": tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.326221Z",
     "iopub.status.busy": "2025-12-07T11:48:49.325956Z",
     "iopub.status.idle": "2025-12-07T11:48:49.330533Z",
     "shell.execute_reply": "2025-12-07T11:48:49.329759Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.326182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display a Sample from the Dataset\n",
    "\n",
    "This cell displays the 4th example (data[3]) from the dataset using the colorize_text_dict function.\n",
    "It shows the prompts (question) in red and responses (answer) in green for easy visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.33174Z",
     "iopub.status.busy": "2025-12-07T11:48:49.331346Z",
     "iopub.status.idle": "2025-12-07T11:48:49.347669Z",
     "shell.execute_reply": "2025-12-07T11:48:49.34712Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.331714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def colorize_text_dict(sample):\n",
    "    \"\"\"\n",
    "    sample: dict with keys 'prompts' and 'responses'\n",
    "    \"\"\"\n",
    "    colored_text = \"\"\n",
    "    colored_text += f\"**<font color='red'>Question:</font>** {sample['prompts']}\\n\\n\"\n",
    "    colored_text += f\"**<font color='green'>Answer:</font>** {sample['responses']}\\n\\n\"\n",
    "    return colored_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.348363Z",
     "iopub.status.busy": "2025-12-07T11:48:49.348184Z",
     "iopub.status.idle": "2025-12-07T11:48:49.363551Z",
     "shell.execute_reply": "2025-12-07T11:48:49.363022Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.348349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.364525Z",
     "iopub.status.busy": "2025-12-07T11:48:49.364291Z",
     "iopub.status.idle": "2025-12-07T11:48:49.381516Z",
     "shell.execute_reply": "2025-12-07T11:48:49.380962Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.364504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(colorize_text_dict(data[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Sample Response\n",
    "\n",
    "This cell demonstrates how the Gemma3 270M model generates text:\n",
    "\n",
    "We create a prompt with a question (prompts) and an empty response (responses).\n",
    "\n",
    "The model generates a response with gemma_lm.generate.\n",
    "\n",
    "We format the generated answer and display it using colorize_text_dict for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:48:49.382461Z",
     "iopub.status.busy": "2025-12-07T11:48:49.382215Z",
     "iopub.status.idle": "2025-12-07T11:49:08.645408Z",
     "shell.execute_reply": "2025-12-07T11:49:08.644777Z",
     "shell.execute_reply.started": "2025-12-07T11:48:49.382445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = {\n",
    "    \"prompts\":\"What are the treatments for Glaucoma ?\",\n",
    "    \"responses\":\"\"}\n",
    "response = gemma_lm.generate(prompt, max_length=cfg.max_length)\n",
    "\n",
    "answer = {\"prompts\": prompt[\"prompts\"][0], \"responses\": response[len(prompt[\"prompts\"][0]):]}\n",
    "display(Markdown(colorize_text_dict(answer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable LoRA Fine-Tuning\n",
    "\n",
    "This cell enables LoRA (Low-Rank Adaptation) on the Gemma3 model:\n",
    "\n",
    "LoRA allows parameter-efficient fine-tuning by only training low-rank matrices instead of the full model.\n",
    "\n",
    "We set the LoRA rank to cfg.lora_rank as defined in our configuration.\n",
    "\n",
    "After enabling LoRA, we display the model summary to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:49:08.646829Z",
     "iopub.status.busy": "2025-12-07T11:49:08.646283Z",
     "iopub.status.idle": "2025-12-07T11:49:08.797255Z",
     "shell.execute_reply": "2025-12-07T11:49:08.79672Z",
     "shell.execute_reply.started": "2025-12-07T11:49:08.646801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to cfg.lora_rank.\n",
    "gemma_lm.backbone.enable_lora(rank=cfg.lora_rank)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Gemma3 on Medical QA Dataset\n",
    "\n",
    "This cell performs the actual fine-tuning of the model:\n",
    "\n",
    "\n",
    "* Limits input sequences to cfg.max_length to control GPU memory usage.\n",
    "* Uses AdamW optimizer, common for transformer models, with weight decay.Excludes biases and layer norm parameters from weight decay.\n",
    "* Uses SparseCategoricalCrossentropy as the loss function and tracks accuracy.\n",
    "* Batches the dataset according to cfg.batch_size and trains for cfg.epochs epochs.\n",
    "* Training history is saved in the history variable for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T11:49:08.798473Z",
     "iopub.status.busy": "2025-12-07T11:49:08.79818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune on the Medical QA dataset.\n",
    "\n",
    "# Limit the input sequence length to 128 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = cfg.max_length\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "batched_dataset = dataset.batch(cfg.batch_size)\n",
    "\n",
    "history = gemma_lm.fit(batched_dataset, epochs=cfg.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training Performance\n",
    "\n",
    "This cell plots the training loss and accuracy over epochs using Matplotlib:\n",
    "\n",
    "\n",
    "* loss tracks the model’s cross-entropy loss.\n",
    "* accuracy tracks the model’s Sparse Categorical Accuracy.\n",
    "* Separate plots are generated to visualize how the model improved during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "accuracy = history.history['sparse_categorical_accuracy']\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(loss, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(accuracy, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Response After Fine-Tuning\n",
    "\n",
    "This cell tests the fine-tuned Gemma3 model:\n",
    "\n",
    "We create a prompt using a template with a question and empty answer.\n",
    "\n",
    "The model generates a response using gemma_lm.generate.\n",
    "\n",
    "The output is displayed using colorize_text for better readability in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
    "prompt = template.format(\n",
    "    question=\"What are the complications of Paget's Disease of Bone ?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "response = gemma_lm.generate(prompt, max_length=cfg.max_length)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    question=\"What are the treatments for Diabetes ?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "response = gemma_lm.generate(prompt, max_length=cfg.max_length)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    question=\"What are the symptoms of Glaucoma ?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "response = gemma_lm.generate(prompt, max_length=cfg.max_length)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    question=\"What are the treatments for Glaucoma ?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "response = gemma_lm.generate(prompt, max_length=cfg.max_length)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    question=\"What causes High Blood Pressure ?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "response = gemma_lm.generate(prompt, max_length=cfg.max_length)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5314260,
     "sourceId": 8831896,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 279036,
     "modelInstanceId": 410137,
     "sourceId": 521980,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
